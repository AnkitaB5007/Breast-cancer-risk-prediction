{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive model using Support Vector Machine (SVM)\n",
    "\n",
    "Support vector machines (SVMs) learning algorithm will be used to build the predictive model.  SVMs are one of the most popular classification algorithms, and have an elegant way of transforming nonlinear data so that one can use a linear algorithm to fit a linear model to the data (Cortes and Vapnik 1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Load libraries for data processing\n",
    "import pandas as pd #data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "## Supervised learning.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8,4) \n",
    "#plt.rcParams['axes.titlesize'] = 'large'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/clean-data.csv', index_col=False)\n",
    "data.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "#data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sheeroh/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Assign predictors to a variable of ndarray (matrix) type\n",
    "array = data.values\n",
    "X = array[:,1:31]\n",
    "y = array[:,0]\n",
    "\n",
    "#transform the class labels from their original string representation (M and B) into integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Normalize the  data (center around 0 and scale to remove the variance).\n",
    "scaler =StandardScaler()\n",
    "Xs = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with cross-validation\n",
    "\n",
    "As discucced in noteboon [NB3](https://github.com/ShiroJean/Breast-cancer-risk-prediction/blob/master/NB3_DataPreprocesing.ipynb)s plitting the data into test and training sets is crucial to avoid overfitting and, therefore, allows you to generalize to real, previously-unseen data. Cross-validation extends this idea further. Instead of having a single train/test split, we can specify so-called folds so that the data is divided into similarly-sized folds. \n",
    "\n",
    "Training occurs by taking all folds except one – referred to as the holdout sample. On the completion of the training, you test the performance of your fitted model using the holdout sample. The holdout sample is then thrown back with the rest of the other folds, and a different fold is pulled out as the new holdout sample. Training is repeated again with the remaining folds and we measure performance using the holdout sample. This process is repeated until each fold has had a chance to be a test or holdout sample. The expected performance of the classifier, called cross-validation error, is then simply an average of error rates computed on each holdout sample. This process is demonstrated by first performing a standard train/test split, and then computing cross-validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The classifier accuracy score is 0.95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. Divide records in training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.3, random_state=2, stratify=y)\n",
    "\n",
    "# 6. Create an SVM classifier and train it on 70% of the data set.\n",
    "clf = SVC(probability=True)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    " #7. Analyze accuracy of predictions on 30% of the holdout test sample.\n",
    "classifier_score = clf.score(X_test, y_test)\n",
    "print '\\nThe classifier accuracy score is {:03.2f}\\n'.format(classifier_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better measure of prediction accuracy (which you can use as a proxy for “goodness of fit” of the model), you can successively split the data into folds that you will use for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 3-fold cross-validation accuracy score for this classifier is 0.97\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get average of 3-fold cross-validation score using an SVC estimator.\n",
    "n_folds = 3\n",
    "cv_error = np.average(cross_val_score(SVC(), Xs, y, cv=n_folds))\n",
    "print '\\nThe {}-fold cross-validation accuracy score for this classifier is {:.2f}\\n'.format(n_folds, cv_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above evaluations were based on using the entire set of features. You will now employ the correlation-based feature selection strategy to assess the effect of using 100 features which have the best correlation with the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "clf2 = make_pipeline(SelectKBest(f_regression, k=3),SVC(probability=True))\n",
    "\n",
    "scores = cross_val_score(clf2, Xs, y, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.93157895  0.95263158  0.94179894]\n",
      "Average score and uncertainty: (94.20 +- 0.496)%\n"
     ]
    }
   ],
   "source": [
    "print scores\n",
    "avg = (100*np.mean(scores), 100*np.std(scores)/np.sqrt(scores.shape[0]))\n",
    "print \"Average score and uncertainty: (%.2f +- %.3f)%%\"%avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, you can see that only a fraction of the features are required to build a model that performs similarly to models based on using the entire set of features.\n",
    "Feature selection is an important part of the model-building process that you must always pay particular attention to. The details are beyond the scope of this notebook. In the rest of the analysis, you will continue using the entire set of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model Accuracy: Receiver Operating Characteristic (ROC) curve\n",
    "\n",
    "In statistical modeling and machine learning, a commonly-reported performance measure of model accuracy for binary classification problems is Area Under the Curve (AUC).\n",
    "\n",
    "To understand what information the ROC curve conveys, consider the so-called confusion matrix that essentially is a two-dimensional table where the classifier model is on one axis (vertical), and ground truth is on the other (horizontal) axis, as shown below. Either of these axes can take two values (as depicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Model says \"+\" |Model says  \"-\" \n",
    "--- | --- | ---\n",
    "`True positive` | `False negative` | ** Actual: \"+\" **\n",
    " `False positive`  | `True negative` |  Actual: \"-\"\n",
    " In an ROC curve, you plot “True Positive Rate” on the Y-axis and “False Positive Rate” on the X-axis, where the values “true positive”, “false negative”, “false positive”, and “true negative” are events (or their probabilities) as described above. The rates are defined according to the following:\n",
    "> * True positive rate (or sensitivity)}: tpr = tp / (tp + fn)\n",
    "> * False positive rate:       fpr = fp / (fp + tn)\n",
    "> * True negative rate (or specificity): tnr = tn / (fp + tn)\n",
    "\n",
    "In all definitions, the denominator is a row margin in the above confusion matrix. Thus,one can  express\n",
    "* the true positive rate (tpr) as the probability that the model says \"+\" when the real value is indeed \"+\" (i.e., a conditional probability). However, this does not tell you how likely you are to be correct when calling \"+\" (i.e., the probability of a true positive, conditioned on the test result being \"+\").\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The confusion matrix helps visualize the performance of the algorithm.\n",
    "y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "#print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATgAAAE8CAYAAABU7q18AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113e91690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.99      0.96       107\n",
      "          1       0.98      0.89      0.93        64\n",
      "\n",
      "avg / total       0.95      0.95      0.95       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.matshow(cm, cmap=plt.cm.Reds, alpha=0.3)\n",
    "for i in range(cm.shape[0]):\n",
    "     for j in range(cm.shape[1]):\n",
    "         ax.text(x=j, y=i,\n",
    "                s=cm[i, j], \n",
    "                va='center', ha='center')\n",
    "plt.xlabel('Predicted Values', )\n",
    "plt.ylabel('Actual Values')\n",
    "plt.show()\n",
    "print(classification_report(y_test, y_pred ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation \n",
    "There are two possible predicted classes: \"1\" and \"0\". Malignant = 1 (indicates prescence of cancer cells) and Benign\n",
    "= 0 (indicates abscence).\n",
    "\n",
    "* The classifier made a total of 174 predictions (i.e 174 patients were being tested for the presence breast cancer).\n",
    "* Out of those 174 cases, the classifier predicted \"yes\" 58 times, and \"no\" 113 times.\n",
    "* In reality, 64 patients in the sample have the disease, and 107 patients do not.\n",
    "\n",
    "#### Rates as computed from the confusion matrix\n",
    "1. **Accuracy**: Overall, how often is the classifier correct?\n",
    "(TP+TN)/total = (100+50)/165 = 0.91\n",
    "2. Misclassification Rate: Overall, how often is it wrong?\n",
    "(FP+FN)/total = (10+5)/165 = 0.09\n",
    "equivalent to 1 minus Accuracy\n",
    "also known as \"Error Rate\"\n",
    "3. True Positive Rate: When it's actually yes, how often does it predict yes?\n",
    "TP/actual yes = 100/105 = 0.95\n",
    "also known as \"Sensitivity\" or \"Recall\"\n",
    "4. False Positive Rate: When it's actually no, how often does it predict yes?\n",
    "FP/actual no = 10/60 = 0.17\n",
    "5. Specificity: When it's actually no, how often does it predict no?\n",
    "TN/actual no = 50/60 = 0.83\n",
    "equivalent to 1 minus False Positive Rate\n",
    "6. Precision: When it predicts yes, how often is it correct?\n",
    "TP/predicted yes = 100/110 = 0.91\n",
    "7. Prevalence: How often does the yes condition actually occur in our sample?\n",
    "actual yes/total = 105/165 = 0.64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "# Plot the receiver operating characteristic curve (ROC).\n",
    "plt.figure(figsize=(10,8))\n",
    "probas_ = clf.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, probas_[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, lw=1, label='ROC fold (area = %0.2f)' % (roc_auc))\n",
    "plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.axes().set_aspect(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To interpret the ROC correctly, consider what the points that lie along the diagonal represent. For these situations, there is an equal chance of \"+\" and \"-\" happening. Therefore, this is not that different from making a prediction by tossing of an unbiased coin. Put simply, the classification model is random.\n",
    "\n",
    "* For the points above the diagonal, tpr > fpr, and the model says that you are in a zone where you are performing better than random. For example, assume tpr = 0.6 and fpr = 0.2.Then, the probability of being in the true positive group is $(0.6 / (0.6 + 0.2)) = 75\\%$. Furthermore, holding fpr constant, it is easy to see that the more vertically above the diagonal you are positioned, the better the classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cortes, Corinna, and Vladimir Vapnik. 1995. “Support-Vector Networks.” Machine Learning 20: 273. Accessed September 3, 2016. doi: 10.1023/A:1022627411411.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
